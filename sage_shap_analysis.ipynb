{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import sage\n",
    "\n",
    "# Accelerate sklearn operations on Intel CPUs\n",
    "patch_sklearn()\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "#shap.initjs() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['girls', 'mixed'] # choose 2 from 'boys', 'girls', and 'mixed'\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "fn = f\"data/selected_features_{classes[0]}_{classes[1]}.csv\" # see binary_xgboost_optimizer.py\n",
    "features_df = pd.read_csv(fn, index_col=\"stimulus_id\")\n",
    "\n",
    "# make features names shorter for plotting\n",
    "features_df.columns = [col.replace(\"_Mean\", \"\").replace(\"Mean\", \"\") for col in features_df.columns]\n",
    "features_df.columns = [col.replace(\"_peak_Peak\", \"_Peak\") for col in features_df.columns]\n",
    "\n",
    "# now make them all long 25 characters (the longest), by adding spaces before the feature name\n",
    "features_df.columns = [col.rjust(25) if col != \"target\" else col for col in features_df.columns]\n",
    "\n",
    "# format\n",
    "X = features_df.drop(\"target\", axis=1)\n",
    "y = features_df[\"target\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0, class1 = tuple(label_encoder.inverse_transform([0, 1]))\n",
    "print(f\"Class 0: {class0}\\nClass 1: {class1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the best parameters from an optuna study with 2000 trials (see binary_xgboost_optimizer.py)\n",
    "fn = f\"xgboost_params_2000_{classes[0]}_{classes[1]}.json\"\n",
    "with open(fn) as json_file:\n",
    "    best_params = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset\n",
    "        \"tree_method\": \"exact\",\n",
    "}\n",
    "best_params.update(base_params)\n",
    "\n",
    "model = XGBClassifier(**best_params)\n",
    "loocv = KFold(len(X))\n",
    "y_pred = cross_val_predict(model, X, y, cv=loocv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.inverse_transform([0,1]), columns=label_encoder.inverse_transform([0,1]))\n",
    "\n",
    "pl.figure(figsize=(5.5,4))\n",
    "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "pl.title(f'F1 score:{f1_score(y, y_pred):.2f}')\n",
    "pl.ylabel('True label')\n",
    "pl.xlabel('Predicted label')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(**best_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGE analysis \n",
    "Global feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an imputer to handle missing features\n",
    "imputer = sage.MarginalImputer(model, X.values)\n",
    "\n",
    "# Set up an estimator\n",
    "estimator = sage.PermutationEstimator(imputer, loss='cross entropy', random_state=42)\n",
    "\n",
    "# Calculate SAGE values\n",
    "sage_values = estimator(X.values, y)\n",
    "sage_values.plot(X.columns, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sage_values_df = pd.DataFrame(columns=['feature name','SAGE value'])\n",
    "sage_values_df['feature name'] = X.columns\n",
    "sage_values_df['SAGE value'] = sage_values.values\n",
    "sage_values_df.sort_values(by=['SAGE value'], ascending=False, inplace=True)\n",
    "\n",
    "# give SAGE order to SHAP beeswarm plot\n",
    "sage_ordered_features = sage_values_df['feature name'].tolist()\n",
    "\n",
    "col2num = {col: i for i, col in enumerate(X.columns)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top 10 sage values only\n",
    "pl.figure(figsize=(10,10))\n",
    "sns.barplot(x=\"SAGE value\", y=\"feature name\", data=sage_values_df, order=sage_ordered_features[:10])\n",
    "#pl.title('Feature importance')\n",
    "pl.ylabel('')\n",
    "pl.xticks(fontsize=30)\n",
    "pl.yticks(fontsize=30)\n",
    "pl.xlabel('SAGE value', fontsize=30)\n",
    "pl.savefig(f'sage_top_10_{class0}_{class1}.pdf', format='pdf', bbox_inches='tight')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder features so that mfccs are last (and won't be shown in the SHAP beeswarm plot)\n",
    "ordered_not_mfccs = [f for f in sage_ordered_features if 'mfcc' not in f]\n",
    "ordered_mfccs = [f for f in sage_ordered_features if 'mfcc' in f]\n",
    "re_ordered_features = ordered_not_mfccs + ordered_mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP analysis\n",
    "Local feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, feature_names=X.columns)\n",
    "shap_values = explainer(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the first prediction's explanation\n",
    "# shap.plots.waterfall(shap_values[0], max_display=20) # f(x) = log odds\n",
    "\n",
    "# summarize the effects of all the features\n",
    "ax = shap.plots.beeswarm(\n",
    "    shap_values, \n",
    "    show= False, \n",
    "    order=[col2num[col] for col in re_ordered_features], \n",
    "    max_display=11, \n",
    "    color_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/shap/shap/blob/master/shap/plots/_beeswarm.py\n",
    "\n",
    "import colorcet as cc\n",
    "from matplotlib.colors import to_rgb, LinearSegmentedColormap\n",
    "\n",
    "clist = [to_rgb(c) for c in cc.CET_CBTL1[::-1][30:-120]]\n",
    "cm = LinearSegmentedColormap.from_list(\"\", clist, N=256)\n",
    "display(cm)\n",
    "\n",
    "from shap.plots._labels import labels\n",
    "import scipy\n",
    "\n",
    "color_bar = False\n",
    "max_display = 10\n",
    "all_fontsizes = 30\n",
    "plot_size = (10, 10)\n",
    "row_height = 0.4\n",
    "alpha = 1\n",
    "axis_color=\"#333333\"\n",
    "dots_size = 25\n",
    "color = cm\n",
    "color_bar_label=labels[\"FEATURE_VALUE\"]\n",
    "\n",
    "shap_exp = shap_values\n",
    "# we make a copy here, because later there are places that might modify this array\n",
    "values = np.copy(shap_exp.values)\n",
    "features = shap_exp.data\n",
    "if scipy.sparse.issparse(features):\n",
    "    features = features.toarray()\n",
    "feature_names = shap_exp.feature_names\n",
    "\n",
    "num_features = values.shape[1]\n",
    "\n",
    "feature_order = [col2num[col] for col in re_ordered_features]\n",
    "\n",
    "feature_inds = feature_order[:max_display]\n",
    "\n",
    "# build our y-tick labels\n",
    "yticklabels = [feature_names[i] for i in feature_inds]\n",
    "\n",
    "pl.gcf().set_size_inches(plot_size[0], plot_size[1])\n",
    "pl.axvline(x=0, color=\"#999999\", zorder=-1)\n",
    "\n",
    "# make the beeswarm dots\n",
    "for pos, i in enumerate(reversed(feature_inds)):\n",
    "    pl.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "    shaps = values[:, i]\n",
    "    fvalues = None if features is None else features[:, i]\n",
    "    inds = np.arange(len(shaps))\n",
    "    np.random.shuffle(inds)\n",
    "    if fvalues is not None:\n",
    "        fvalues = fvalues[inds]\n",
    "    shaps = shaps[inds]\n",
    "    colored_feature = True\n",
    "\n",
    "    fvalues = np.array(fvalues, dtype=np.float64)  # make sure this can be numeric\n",
    "\n",
    "    N = len(shaps)\n",
    "    # hspacing = (np.max(shaps) - np.min(shaps)) / 200\n",
    "    # curr_bin = []\n",
    "    nbins = 100\n",
    "    quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n",
    "    inds = np.argsort(quant + np.random.randn(N) * 1e-6)\n",
    "    layer = 0\n",
    "    last_bin = -1\n",
    "    ys = np.zeros(N)\n",
    "    for ind in inds:\n",
    "        if quant[ind] != last_bin:\n",
    "            layer = 0\n",
    "        ys[ind] = np.ceil(layer / 2) * ((layer % 2) * 2 - 1)\n",
    "        layer += 1\n",
    "        last_bin = quant[ind]\n",
    "    ys *= 0.9 * (row_height / np.max(ys + 1))\n",
    "\n",
    "    if features is not None and colored_feature:\n",
    "        # trim the color range, but prevent the color range from collapsing\n",
    "        vmin = np.nanpercentile(fvalues, 5)\n",
    "        vmax = np.nanpercentile(fvalues, 95)\n",
    "        if vmin == vmax:\n",
    "            vmin = np.nanpercentile(fvalues, 1)\n",
    "            vmax = np.nanpercentile(fvalues, 99)\n",
    "            if vmin == vmax:\n",
    "                vmin = np.min(fvalues)\n",
    "                vmax = np.max(fvalues)\n",
    "        if vmin > vmax: # fixes rare numerical precision issues\n",
    "            vmin = vmax\n",
    "\n",
    "        if features.shape[0] != len(shaps):\n",
    "            emsg = \"Feature and SHAP matrices must have the same number of rows!\"\n",
    "            raise DimensionError(emsg)\n",
    "\n",
    "        # plot the nan fvalues in the interaction feature as YELLOW\n",
    "        nan_mask = np.isnan(fvalues)\n",
    "        pl.scatter(shaps[nan_mask], pos + ys[nan_mask], color=\"#FDFD96\",\n",
    "                    s=16, alpha=alpha, linewidth=0,\n",
    "                    zorder=3, rasterized=len(shaps) > 500)\n",
    "\n",
    "        # plot the non-nan fvalues colored by the trimmed feature value\n",
    "        cvals = fvalues[np.invert(nan_mask)].astype(np.float64)\n",
    "        cvals_imp = cvals.copy()\n",
    "        cvals_imp[np.isnan(cvals)] = (vmin + vmax) / 2.0\n",
    "        cvals[cvals_imp > vmax] = vmax\n",
    "        cvals[cvals_imp < vmin] = vmin\n",
    "        pl.scatter(shaps[np.invert(nan_mask)], pos + ys[np.invert(nan_mask)],\n",
    "                    cmap=color, vmin=vmin, vmax=vmax, s=dots_size,\n",
    "                    c=cvals, alpha=alpha, linewidth=0,\n",
    "                    zorder=3, rasterized=len(shaps) > 500)\n",
    "\n",
    "\n",
    "# draw the color bar\n",
    "if color_bar and features is not None:\n",
    "    import matplotlib.cm as cm\n",
    "    m = cm.ScalarMappable(cmap=color)\n",
    "    m.set_array([0, 1])\n",
    "    cb = pl.colorbar(m, ax=pl.gca(), ticks=[0, 1], aspect=40)\n",
    "    cb.set_ticklabels([labels['FEATURE_VALUE_LOW'], labels['FEATURE_VALUE_HIGH']])\n",
    "    cb.set_label(color_bar_label, size=all_fontsizes, labelpad=0)\n",
    "    cb.ax.tick_params(labelsize=all_fontsizes, length=0)\n",
    "    cb.set_alpha(1)\n",
    "    cb.outline.set_visible(False)\n",
    "\n",
    "pl.gca().xaxis.set_ticks_position('bottom')\n",
    "pl.gca().yaxis.set_ticks_position('none')\n",
    "pl.gca().spines['right'].set_visible(False)\n",
    "pl.gca().spines['top'].set_visible(False)\n",
    "pl.gca().spines['left'].set_visible(False)\n",
    "pl.gca().tick_params(color=axis_color, labelcolor=axis_color)\n",
    "pl.yticks(range(len(feature_inds)), reversed(yticklabels), fontsize=all_fontsizes)\n",
    "pl.gca().tick_params('y', length=20, width=0.5, which='major')\n",
    "pl.gca().tick_params('x', labelsize=all_fontsizes)\n",
    "pl.ylim(-1, len(feature_inds))\n",
    "pl.xlabel(\"SHAP value\", fontsize=all_fontsizes)\n",
    "pl.savefig(f'shap_top_10_{class0}_{class1}.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions\n",
    "\n",
    "Note that when there are no interactions present, the SHAP interaction values are just a diagonal matrix with the SHAP values on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get interaction values\n",
    "shap_interaction_values = explainer.shap_interaction_values(X)\n",
    "\n",
    "# Check that SHAP interaction values essentially sum to the marginal predictions\n",
    "pred = model.predict_proba(X)\n",
    "pred_logodds = np.log(pred / (1 - pred))[:, 1] # convert to log odds\n",
    "print(np.abs(shap_interaction_values.sum((1, 2)) + explainer.expected_value - pred_logodds).max())\n",
    "\n",
    "# get mean absolute value of interaction values\n",
    "shap_interaction_mean = np.abs(shap_interaction_values).mean(0)\n",
    "shap_interaction_mean = np.round(shap_interaction_mean, decimals=1)\n",
    "shap_interaction_df = pd.DataFrame(shap_interaction_mean, index=X.columns, columns=X.columns)\n",
    "\n",
    "# reorder rows and columns according to SAGE order\n",
    "shap_interaction_df = shap_interaction_df.reindex(sage_ordered_features)\n",
    "shap_interaction_df = shap_interaction_df[sage_ordered_features]\n",
    "\n",
    "# plot heatmap\n",
    "pl.figure(figsize=(10,10))\n",
    "sns.heatmap(shap_interaction_df, annot=True, cmap='Blues', fmt='g')\n",
    "pl.title('Interaction effects')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainable_modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
